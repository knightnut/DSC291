{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the project\n",
    "The goal is to use the variable WT08 analyzing if there is a relationship between COVID-19 and US air quality in 2020.According to our plan, the experimental design is as follows: \n",
    "\n",
    "1. Extract daily values of variables over January - April of all years to the present worldwide.\n",
    "2. Quantify historic variability in each variable at a site-by-site level: e.g., mean and CI (parametric or non-parametric quantile intervals) by site over all years to 2019. This might depend on data scarcity/availability.\n",
    "3. Evaluate placement of 2020 observations relative to long-term variability using appropriate statistical test.\n",
    "4. Potential use of PCA: across all sites (not site by site), could look and see if components responsible for greatest variation in 2020 data look different than prior years (not sure if thatâ€™s possible - might be for precip and temp, prob not for smoke/haze and fog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import urllib\n",
    "import math\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local[4]\")\n",
    "#sc.version\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "import pyspark.sql\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the data\n",
    "Just make some small tests. I have downloaded NOAA GHCN-D data (year 2017-2020) from AWS and saved them in the directory ~/weather_data/test\n",
    "The stations text file is saved in the same directory.\n",
    "The yearly files stores wheather data of the whole world, while we are mainly interested in the WT08 variable of US from January to April each year.So first we need to do some data cleaning.Take year 2020 for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-------+----+--------+-------+----+----+----+----+----+----+-------+--------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+\n",
      "|        _c0| _c1|    _c2| _c3|     _c4|    _c5| _c6| _c7| _c8| _c9|_c10|_c11|   _c12|    _c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|_c21|_c22|_c23|_c24|_c25|_c26|_c27|_c28|_c29|_c30| _c31|_c32|_c33|_c34|_c35|_c36|_c37|\n",
      "+-----------+----+-------+----+--------+-------+----+----+----+----+----+----+-------+--------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+\n",
      "|ACW00011604|null|17.1167|null|-61.7833|   null|null|10.1|null|null|null|  ST|  JOHNS|COOLIDGE| FLD|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null| null|null|null|null|null|null|null|\n",
      "|ACW00011647|null|17.1333|null|-61.7833|   null|null|19.2|null|null|null|  ST|  JOHNS|    null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null| null|null|null|null|null|null|null|\n",
      "|AE000041196|null|25.3330|null|    null|55.5170|null|null|34.0|null|null|null|SHARJAH|  INTER.|AIRP|null|null|null|null|null|null|null|null|null|null|null| GSN|null|null|null|null|41196|null|null|null|null|null|null|\n",
      "|AEM00041194|null|25.2550|null|    null|55.3640|null|null|10.4|null|null|null|  DUBAI|    INTL|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null| null|null|null|null|null|null|null|\n",
      "+-----------+----+-------+----+--------+-------+----+----+----+----+----+----+-------+--------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations = sqlContext.read.csv(\"/home/fyjj/weather_data/test/stations.txt\", sep=\" \", mode=\"PERMISSIVE\", header=False)\n",
    "stations.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+\n",
      "|        _c0|_c7|\n",
      "+-----------+---+\n",
      "|US009052008| SD|\n",
      "|US10RMHS145|1.6|\n",
      "|US10adam001| NE|\n",
      "|US10adam002| NE|\n",
      "+-----------+---+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "stations = stations.filter(col(\"_c0\").like(\"US%\"))\n",
    "stations = stations.select(col(\"_c0\"), col(\"_c7\"))\n",
    "stations.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----+---+----+----+---+----+\n",
      "|        _c0|     _c1| _c2|_c3| _c4| _c5|_c6| _c7|\n",
      "+-----------+--------+----+---+----+----+---+----+\n",
      "|US1FLSL0019|20200101|PRCP|  0|null|null|  N|null|\n",
      "|US1FLSL0019|20200101|SNOW|  0|null|null|  N|null|\n",
      "|US1NVNY0012|20200101|PRCP|  0|null|null|  N|null|\n",
      "|US1NVNY0012|20200101|SNOW|  0|null|null|  N|null|\n",
      "+-----------+--------+----+---+----+----+---+----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = '/home/fyjj/weather_data/test/2020.csv'\n",
    "weather_df = sqlContext.read.format(\"csv\").option(\"header\",\"False\").load(filename)\n",
    "weather_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+\n",
      "|    station|    data|element|\n",
      "+-----------+--------+-------+\n",
      "|US1FLSL0019|20200101|   PRCP|\n",
      "|US1FLSL0019|20200101|   SNOW|\n",
      "|US1NVNY0012|20200101|   PRCP|\n",
      "|US1NVNY0012|20200101|   SNOW|\n",
      "+-----------+--------+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df = weather_df.drop(\"_c3\").drop(\"_c4\").drop(\"_c5\").drop(\"_c6\").drop(\"_c7\")\n",
    "weather_df = weather_df.withColumnRenamed(\"_c0\",\"station\").withColumnRenamed(\"_c1\",\"data\").withColumnRenamed(\"_c2\",\"element\")\n",
    "weather_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+\n",
      "|    station|    data|element|\n",
      "+-----------+--------+-------+\n",
      "|USW00013927|20200101|   WT08|\n",
      "|USW00023061|20200101|   WT08|\n",
      "|USW00026615|20200101|   WT08|\n",
      "|USW00053120|20200101|   WT08|\n",
      "+-----------+--------+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df = weather_df.filter(weather_df[\"element\"]==\"WT08\")\n",
    "weather_df = weather_df.filter(weather_df[\"data\"]<20200501)\n",
    "weather_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|    station|count|\n",
      "+-----------+-----+\n",
      "|USW00023234|  112|\n",
      "|USW00012972|   91|\n",
      "|USW00003166|   81|\n",
      "|USW00094938|   80|\n",
      "|USW00094931|   78|\n",
      "+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.registerDataFrameAsTable(weather_df,'weather_df')\n",
    "sqlContext.sql('select station, count(station) as count from weather_df group by station order by count desc').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to join the weather data with the station data to determine the specific state/city the station is in. But don't know how to do it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
